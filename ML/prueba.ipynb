{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from model import LSTM"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T04:03:08.471521300Z",
     "start_time": "2024-01-29T04:03:04.787680100Z"
    }
   },
   "id": "178d52d3a2c025e9"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'model.LSTM'>\n"
     ]
    }
   ],
   "source": [
    "print(LSTM)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T15:15:06.865136400Z",
     "start_time": "2024-01-28T15:15:06.844997400Z"
    }
   },
   "id": "1bd26d4de619d872"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import time\n",
    "tiempo_inicio = time.time()\n",
    "tiempo_fin = time.time()\n",
    "tiempo_ejecucion = (tiempo_fin - tiempo_inicio) * 60\n",
    "\n",
    "def mes(row):\n",
    "    if row['mes'] < 10:\n",
    "        return '0' + str(row['mes'])\n",
    "    else:\n",
    "        return str(row['mes'])\n",
    "\n",
    "###---------------------------------------------------------\n",
    "df = pd.read_pickle('./Data/lstm_mensual_2.pkl')\n",
    "df['mes'] = df.apply(lambda row: mes(row), axis=1)\n",
    "df['Fecha_Inicio'] = df['year'].astype(str) + '-' + df['mes'].astype(str) + '-01'\n",
    "\n",
    "df.Fecha_Inicio = pd.to_datetime(df.Fecha_Inicio)\n",
    "df_exito = df[(df.Razon_Social_Cliente == 'Almacenes Exito Sa')& (df.Fecha_Inicio < '2023-11-01')].groupby(['Fecha_Inicio'],as_index=False).COP.sum()\n",
    "df_exito = df_exito.set_index('Fecha_Inicio')\n",
    "df_exito = df_exito.sort_index(ascending=False)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T04:03:29.836512200Z",
     "start_time": "2024-01-29T04:03:29.773682800Z"
    }
   },
   "id": "7c4cb662d5ced035"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                       COP\nFecha_Inicio              \n2023-10-01    1.718048e+09\n2023-09-01    1.776914e+09\n2023-08-01    1.626946e+09\n2023-07-01    1.987764e+09\n2023-06-01    1.820727e+09\n...                    ...\n2018-05-01    2.541605e+09\n2018-04-01    1.881336e+09\n2018-03-01    2.486371e+09\n2018-02-01    1.977903e+09\n2018-01-01    2.463545e+09\n\n[70 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>COP</th>\n    </tr>\n    <tr>\n      <th>Fecha_Inicio</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-10-01</th>\n      <td>1.718048e+09</td>\n    </tr>\n    <tr>\n      <th>2023-09-01</th>\n      <td>1.776914e+09</td>\n    </tr>\n    <tr>\n      <th>2023-08-01</th>\n      <td>1.626946e+09</td>\n    </tr>\n    <tr>\n      <th>2023-07-01</th>\n      <td>1.987764e+09</td>\n    </tr>\n    <tr>\n      <th>2023-06-01</th>\n      <td>1.820727e+09</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2018-05-01</th>\n      <td>2.541605e+09</td>\n    </tr>\n    <tr>\n      <th>2018-04-01</th>\n      <td>1.881336e+09</td>\n    </tr>\n    <tr>\n      <th>2018-03-01</th>\n      <td>2.486371e+09</td>\n    </tr>\n    <tr>\n      <th>2018-02-01</th>\n      <td>1.977903e+09</td>\n    </tr>\n    <tr>\n      <th>2018-01-01</th>\n      <td>2.463545e+09</td>\n    </tr>\n  </tbody>\n</table>\n<p>70 rows Ã— 1 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exito"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T04:03:31.692992500Z",
     "start_time": "2024-01-29T04:03:31.679173500Z"
    }
   },
   "id": "da59e2f8695b24"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 5, 1])\n",
      "torch.Size([65, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Local\\Temp\\ipykernel_12080\\4028785214.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  return torch.tensor(sequences), torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, seq_length, num_labels):\n",
    "    sequences, labels = [], []\n",
    "    for i in range(len(data) - seq_length - num_labels + 1):\n",
    "        seq = np.array(data[i:i + seq_length])\n",
    "        label = np.array(data[i + seq_length:(i + seq_length) + num_labels])\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return torch.tensor(sequences), torch.tensor(labels)\n",
    "sequences, labels = create_sequences(df_exito, seq_length=5, num_labels=1)\n",
    "print(sequences.shape)\n",
    "print(labels.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T04:03:38.456775900Z",
     "start_time": "2024-01-29T04:03:38.436481500Z"
    }
   },
   "id": "b39789facaa7c42c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1.6165e+09]],\n\n        [[1.4721e+09]],\n\n        [[1.6540e+09]],\n\n        [[1.5681e+09]],\n\n        [[1.7249e+09]],\n\n        [[2.2489e+09]],\n\n        [[1.5393e+09]],\n\n        [[1.8544e+09]],\n\n        [[1.6759e+09]],\n\n        [[1.7008e+09]],\n\n        [[2.0970e+09]],\n\n        [[1.8522e+09]],\n\n        [[1.9160e+09]],\n\n        [[1.8361e+09]],\n\n        [[2.0682e+09]],\n\n        [[1.8263e+09]],\n\n        [[1.9797e+09]],\n\n        [[2.7017e+09]],\n\n        [[1.7473e+09]],\n\n        [[1.9959e+09]],\n\n        [[1.5206e+09]],\n\n        [[1.5670e+09]],\n\n        [[1.8764e+09]],\n\n        [[1.5186e+09]],\n\n        [[1.4093e+09]],\n\n        [[1.2366e+09]],\n\n        [[1.7932e+09]],\n\n        [[1.6227e+09]],\n\n        [[1.4827e+09]],\n\n        [[2.2869e+09]],\n\n        [[1.6814e+09]],\n\n        [[2.0036e+09]],\n\n        [[1.6714e+09]],\n\n        [[1.4248e+09]],\n\n        [[1.7119e+09]],\n\n        [[1.6661e+09]],\n\n        [[1.9559e+09]],\n\n        [[1.4811e+09]],\n\n        [[2.7138e+09]],\n\n        [[1.9235e+09]],\n\n        [[2.1088e+09]],\n\n        [[3.1727e+09]],\n\n        [[1.7456e+09]],\n\n        [[1.9603e+09]],\n\n        [[1.9329e+09]],\n\n        [[1.9866e+09]],\n\n        [[2.1831e+09]],\n\n        [[2.2750e+09]],\n\n        [[2.3358e+09]],\n\n        [[1.9049e+09]],\n\n        [[2.4089e+09]],\n\n        [[2.0997e+09]],\n\n        [[2.2799e+09]],\n\n        [[3.6029e+09]],\n\n        [[1.8985e+09]],\n\n        [[2.1904e+09]],\n\n        [[2.4147e+09]],\n\n        [[1.9410e+09]],\n\n        [[2.2531e+09]],\n\n        [[2.2007e+09]],\n\n        [[2.5416e+09]],\n\n        [[1.8813e+09]],\n\n        [[2.4864e+09]],\n\n        [[1.9779e+09]],\n\n        [[2.4635e+09]]], dtype=torch.float64)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T04:03:44.467865300Z",
     "start_time": "2024-01-29T04:03:44.461254600Z"
    }
   },
   "id": "eeedba3b0d11fe4a"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1.7180e+09],\n         [1.7769e+09],\n         [1.6269e+09],\n         [1.9878e+09],\n         [1.8207e+09]],\n\n        [[1.7769e+09],\n         [1.6269e+09],\n         [1.9878e+09],\n         [1.8207e+09],\n         [1.6165e+09]],\n\n        [[1.6269e+09],\n         [1.9878e+09],\n         [1.8207e+09],\n         [1.6165e+09],\n         [1.4721e+09]],\n\n        [[1.9878e+09],\n         [1.8207e+09],\n         [1.6165e+09],\n         [1.4721e+09],\n         [1.6540e+09]],\n\n        [[1.8207e+09],\n         [1.6165e+09],\n         [1.4721e+09],\n         [1.6540e+09],\n         [1.5681e+09]],\n\n        [[1.6165e+09],\n         [1.4721e+09],\n         [1.6540e+09],\n         [1.5681e+09],\n         [1.7249e+09]],\n\n        [[1.4721e+09],\n         [1.6540e+09],\n         [1.5681e+09],\n         [1.7249e+09],\n         [2.2489e+09]],\n\n        [[1.6540e+09],\n         [1.5681e+09],\n         [1.7249e+09],\n         [2.2489e+09],\n         [1.5393e+09]],\n\n        [[1.5681e+09],\n         [1.7249e+09],\n         [2.2489e+09],\n         [1.5393e+09],\n         [1.8544e+09]],\n\n        [[1.7249e+09],\n         [2.2489e+09],\n         [1.5393e+09],\n         [1.8544e+09],\n         [1.6759e+09]],\n\n        [[2.2489e+09],\n         [1.5393e+09],\n         [1.8544e+09],\n         [1.6759e+09],\n         [1.7008e+09]],\n\n        [[1.5393e+09],\n         [1.8544e+09],\n         [1.6759e+09],\n         [1.7008e+09],\n         [2.0970e+09]],\n\n        [[1.8544e+09],\n         [1.6759e+09],\n         [1.7008e+09],\n         [2.0970e+09],\n         [1.8522e+09]],\n\n        [[1.6759e+09],\n         [1.7008e+09],\n         [2.0970e+09],\n         [1.8522e+09],\n         [1.9160e+09]],\n\n        [[1.7008e+09],\n         [2.0970e+09],\n         [1.8522e+09],\n         [1.9160e+09],\n         [1.8361e+09]],\n\n        [[2.0970e+09],\n         [1.8522e+09],\n         [1.9160e+09],\n         [1.8361e+09],\n         [2.0682e+09]],\n\n        [[1.8522e+09],\n         [1.9160e+09],\n         [1.8361e+09],\n         [2.0682e+09],\n         [1.8263e+09]],\n\n        [[1.9160e+09],\n         [1.8361e+09],\n         [2.0682e+09],\n         [1.8263e+09],\n         [1.9797e+09]],\n\n        [[1.8361e+09],\n         [2.0682e+09],\n         [1.8263e+09],\n         [1.9797e+09],\n         [2.7017e+09]],\n\n        [[2.0682e+09],\n         [1.8263e+09],\n         [1.9797e+09],\n         [2.7017e+09],\n         [1.7473e+09]],\n\n        [[1.8263e+09],\n         [1.9797e+09],\n         [2.7017e+09],\n         [1.7473e+09],\n         [1.9959e+09]],\n\n        [[1.9797e+09],\n         [2.7017e+09],\n         [1.7473e+09],\n         [1.9959e+09],\n         [1.5206e+09]],\n\n        [[2.7017e+09],\n         [1.7473e+09],\n         [1.9959e+09],\n         [1.5206e+09],\n         [1.5670e+09]],\n\n        [[1.7473e+09],\n         [1.9959e+09],\n         [1.5206e+09],\n         [1.5670e+09],\n         [1.8764e+09]],\n\n        [[1.9959e+09],\n         [1.5206e+09],\n         [1.5670e+09],\n         [1.8764e+09],\n         [1.5186e+09]],\n\n        [[1.5206e+09],\n         [1.5670e+09],\n         [1.8764e+09],\n         [1.5186e+09],\n         [1.4093e+09]],\n\n        [[1.5670e+09],\n         [1.8764e+09],\n         [1.5186e+09],\n         [1.4093e+09],\n         [1.2366e+09]],\n\n        [[1.8764e+09],\n         [1.5186e+09],\n         [1.4093e+09],\n         [1.2366e+09],\n         [1.7932e+09]],\n\n        [[1.5186e+09],\n         [1.4093e+09],\n         [1.2366e+09],\n         [1.7932e+09],\n         [1.6227e+09]],\n\n        [[1.4093e+09],\n         [1.2366e+09],\n         [1.7932e+09],\n         [1.6227e+09],\n         [1.4827e+09]],\n\n        [[1.2366e+09],\n         [1.7932e+09],\n         [1.6227e+09],\n         [1.4827e+09],\n         [2.2869e+09]],\n\n        [[1.7932e+09],\n         [1.6227e+09],\n         [1.4827e+09],\n         [2.2869e+09],\n         [1.6814e+09]],\n\n        [[1.6227e+09],\n         [1.4827e+09],\n         [2.2869e+09],\n         [1.6814e+09],\n         [2.0036e+09]],\n\n        [[1.4827e+09],\n         [2.2869e+09],\n         [1.6814e+09],\n         [2.0036e+09],\n         [1.6714e+09]],\n\n        [[2.2869e+09],\n         [1.6814e+09],\n         [2.0036e+09],\n         [1.6714e+09],\n         [1.4248e+09]],\n\n        [[1.6814e+09],\n         [2.0036e+09],\n         [1.6714e+09],\n         [1.4248e+09],\n         [1.7119e+09]],\n\n        [[2.0036e+09],\n         [1.6714e+09],\n         [1.4248e+09],\n         [1.7119e+09],\n         [1.6661e+09]],\n\n        [[1.6714e+09],\n         [1.4248e+09],\n         [1.7119e+09],\n         [1.6661e+09],\n         [1.9559e+09]],\n\n        [[1.4248e+09],\n         [1.7119e+09],\n         [1.6661e+09],\n         [1.9559e+09],\n         [1.4811e+09]],\n\n        [[1.7119e+09],\n         [1.6661e+09],\n         [1.9559e+09],\n         [1.4811e+09],\n         [2.7138e+09]],\n\n        [[1.6661e+09],\n         [1.9559e+09],\n         [1.4811e+09],\n         [2.7138e+09],\n         [1.9235e+09]],\n\n        [[1.9559e+09],\n         [1.4811e+09],\n         [2.7138e+09],\n         [1.9235e+09],\n         [2.1088e+09]],\n\n        [[1.4811e+09],\n         [2.7138e+09],\n         [1.9235e+09],\n         [2.1088e+09],\n         [3.1727e+09]],\n\n        [[2.7138e+09],\n         [1.9235e+09],\n         [2.1088e+09],\n         [3.1727e+09],\n         [1.7456e+09]],\n\n        [[1.9235e+09],\n         [2.1088e+09],\n         [3.1727e+09],\n         [1.7456e+09],\n         [1.9603e+09]],\n\n        [[2.1088e+09],\n         [3.1727e+09],\n         [1.7456e+09],\n         [1.9603e+09],\n         [1.9329e+09]],\n\n        [[3.1727e+09],\n         [1.7456e+09],\n         [1.9603e+09],\n         [1.9329e+09],\n         [1.9866e+09]],\n\n        [[1.7456e+09],\n         [1.9603e+09],\n         [1.9329e+09],\n         [1.9866e+09],\n         [2.1831e+09]],\n\n        [[1.9603e+09],\n         [1.9329e+09],\n         [1.9866e+09],\n         [2.1831e+09],\n         [2.2750e+09]],\n\n        [[1.9329e+09],\n         [1.9866e+09],\n         [2.1831e+09],\n         [2.2750e+09],\n         [2.3358e+09]],\n\n        [[1.9866e+09],\n         [2.1831e+09],\n         [2.2750e+09],\n         [2.3358e+09],\n         [1.9049e+09]],\n\n        [[2.1831e+09],\n         [2.2750e+09],\n         [2.3358e+09],\n         [1.9049e+09],\n         [2.4089e+09]],\n\n        [[2.2750e+09],\n         [2.3358e+09],\n         [1.9049e+09],\n         [2.4089e+09],\n         [2.0997e+09]],\n\n        [[2.3358e+09],\n         [1.9049e+09],\n         [2.4089e+09],\n         [2.0997e+09],\n         [2.2799e+09]],\n\n        [[1.9049e+09],\n         [2.4089e+09],\n         [2.0997e+09],\n         [2.2799e+09],\n         [3.6029e+09]],\n\n        [[2.4089e+09],\n         [2.0997e+09],\n         [2.2799e+09],\n         [3.6029e+09],\n         [1.8985e+09]],\n\n        [[2.0997e+09],\n         [2.2799e+09],\n         [3.6029e+09],\n         [1.8985e+09],\n         [2.1904e+09]],\n\n        [[2.2799e+09],\n         [3.6029e+09],\n         [1.8985e+09],\n         [2.1904e+09],\n         [2.4147e+09]],\n\n        [[3.6029e+09],\n         [1.8985e+09],\n         [2.1904e+09],\n         [2.4147e+09],\n         [1.9410e+09]],\n\n        [[1.8985e+09],\n         [2.1904e+09],\n         [2.4147e+09],\n         [1.9410e+09],\n         [2.2531e+09]],\n\n        [[2.1904e+09],\n         [2.4147e+09],\n         [1.9410e+09],\n         [2.2531e+09],\n         [2.2007e+09]],\n\n        [[2.4147e+09],\n         [1.9410e+09],\n         [2.2531e+09],\n         [2.2007e+09],\n         [2.5416e+09]],\n\n        [[1.9410e+09],\n         [2.2531e+09],\n         [2.2007e+09],\n         [2.5416e+09],\n         [1.8813e+09]],\n\n        [[2.2531e+09],\n         [2.2007e+09],\n         [2.5416e+09],\n         [1.8813e+09],\n         [2.4864e+09]],\n\n        [[2.2007e+09],\n         [2.5416e+09],\n         [1.8813e+09],\n         [2.4864e+09],\n         [1.9779e+09]]], dtype=torch.float64)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T16:20:23.528026400Z",
     "start_time": "2024-01-28T16:20:23.515544500Z"
    }
   },
   "id": "2d11b33d91fc3d95"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 5, 1])\n",
      "torch.Size([52, 1, 1])\n",
      "torch.Size([13, 5, 1])\n",
      "torch.Size([13, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# splitting data to test and train\n",
    "train_size = int(len(sequences) * 0.8)\n",
    "test_size = len(sequences) - train_size\n",
    "train_sequences, train_labels = sequences[:train_size], labels[:train_size]\n",
    "test_sequences, test_labels = sequences[train_size:], labels[train_size:]\n",
    "print(train_sequences.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_sequences.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "# create a dataset, for implementing the lstm algorithm\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.labels[index]\n",
    "    \n",
    "train_dataset = Dataset(train_sequences, train_labels)\n",
    "test_dataset = Dataset(test_sequences, test_labels)\n",
    "# upload to dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T04:03:51.464779200Z",
     "start_time": "2024-01-29T04:03:51.453622600Z"
    }
   },
   "id": "bdfd13a4be763b32"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# algoritm for lstm for 3 features that will be the inputs and 2 features that will be the outputs\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        self.hidden_cell = (torch.zeros(self.num_layers,input_seq.size(0),self.hidden_layer_size),\n",
    "                            torch.zeros(self.num_layers,input_seq.size(0),self.hidden_layer_size))\n",
    "        # AsegÃºrate de que la dimensiÃ³n de entrada sea correcta\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), -1, 1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T04:04:19.454753500Z",
     "start_time": "2024-01-29T04:04:19.451555100Z"
    }
   },
   "id": "d40ffbf335ee61ad"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([52, 1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 3661960035069591552.00000000\n",
      "epoch:   3 loss: 3661960035069591552.00000000\n",
      "epoch:   5 loss: 3661960035069591552.00000000\n",
      "epoch:   7 loss: 3661959760191684608.00000000\n",
      "epoch:   9 loss: 3661959760191684608.00000000\n",
      "epoch:  11 loss: 3661960309947498496.00000000\n",
      "epoch:  13 loss: 3661960035069591552.00000000\n",
      "epoch:  15 loss: 3661959760191684608.00000000\n",
      "epoch:  17 loss: 3661959760191684608.00000000\n",
      "epoch:  19 loss: 3661960035069591552.00000000\n",
      "epoch:  21 loss: 3661960035069591552.00000000\n",
      "epoch:  23 loss: 3661959760191684608.00000000\n",
      "epoch:  25 loss: 3661960035069591552.00000000\n",
      "epoch:  27 loss: 3661960035069591552.00000000\n",
      "epoch:  29 loss: 3661960035069591552.00000000\n",
      "epoch:  31 loss: 3661960035069591552.00000000\n",
      "epoch:  33 loss: 3661960035069591552.00000000\n",
      "epoch:  35 loss: 3661959760191684608.00000000\n",
      "epoch:  37 loss: 3661960035069591552.00000000\n",
      "epoch:  39 loss: 3661959760191684608.00000000\n",
      "epoch:  41 loss: 3661959760191684608.00000000\n",
      "epoch:  43 loss: 3661960035069591552.00000000\n",
      "epoch:  45 loss: 3661960309947498496.00000000\n",
      "epoch:  47 loss: 3661960035069591552.00000000\n",
      "epoch:  49 loss: 3661959760191684608.00000000\n",
      "epoch:  51 loss: 3661960035069591552.00000000\n",
      "epoch:  53 loss: 3661959760191684608.00000000\n",
      "epoch:  55 loss: 3661960309947498496.00000000\n",
      "epoch:  57 loss: 3661959760191684608.00000000\n",
      "epoch:  59 loss: 3661960035069591552.00000000\n",
      "epoch:  61 loss: 3661959760191684608.00000000\n",
      "epoch:  63 loss: 3661959760191684608.00000000\n",
      "epoch:  65 loss: 3661960035069591552.00000000\n",
      "epoch:  67 loss: 3661959760191684608.00000000\n",
      "epoch:  69 loss: 3661960035069591552.00000000\n",
      "epoch:  71 loss: 3661960035069591552.00000000\n",
      "epoch:  73 loss: 3661959760191684608.00000000\n",
      "epoch:  75 loss: 3661959760191684608.00000000\n",
      "epoch:  77 loss: 3661959760191684608.00000000\n",
      "epoch:  79 loss: 3661959760191684608.00000000\n",
      "epoch:  81 loss: 3661959760191684608.00000000\n",
      "epoch:  83 loss: 3661959760191684608.00000000\n",
      "epoch:  85 loss: 3661959760191684608.00000000\n",
      "epoch:  87 loss: 3661959760191684608.00000000\n",
      "epoch:  89 loss: 3661959760191684608.00000000\n",
      "epoch:  91 loss: 3661960035069591552.00000000\n",
      "epoch:  93 loss: 3661959760191684608.00000000\n",
      "epoch:  95 loss: 3661960035069591552.00000000\n",
      "epoch:  97 loss: 3661960035069591552.00000000\n",
      "epoch:  99 loss: 3661960035069591552.00000000\n",
      "epoch: 101 loss: 3661959760191684608.00000000\n",
      "epoch: 103 loss: 3661959760191684608.00000000\n",
      "epoch: 105 loss: 3661960035069591552.00000000\n",
      "epoch: 107 loss: 3661959760191684608.00000000\n",
      "epoch: 109 loss: 3661959760191684608.00000000\n",
      "epoch: 111 loss: 3661959760191684608.00000000\n",
      "epoch: 113 loss: 3661960035069591552.00000000\n",
      "epoch: 115 loss: 3661960035069591552.00000000\n",
      "epoch: 117 loss: 3661960035069591552.00000000\n",
      "epoch: 119 loss: 3661960035069591552.00000000\n",
      "epoch: 121 loss: 3661960035069591552.00000000\n",
      "epoch: 123 loss: 3661960309947498496.00000000\n",
      "epoch: 125 loss: 3661960035069591552.00000000\n",
      "epoch: 127 loss: 3661959760191684608.00000000\n",
      "epoch: 129 loss: 3661959760191684608.00000000\n",
      "epoch: 131 loss: 3661959760191684608.00000000\n",
      "epoch: 133 loss: 3661959760191684608.00000000\n",
      "epoch: 135 loss: 3661960035069591552.00000000\n",
      "epoch: 137 loss: 3661960035069591552.00000000\n",
      "epoch: 139 loss: 3661960035069591552.00000000\n",
      "epoch: 141 loss: 3661959760191684608.00000000\n",
      "epoch: 143 loss: 3661959760191684608.00000000\n",
      "epoch: 145 loss: 3661959760191684608.00000000\n",
      "epoch: 147 loss: 3661959760191684608.00000000\n",
      "epoch: 149 loss: 3661960035069591552.00000000\n",
      "epoch: 151 loss: 3661959760191684608.00000000\n",
      "epoch: 153 loss: 3661960035069591552.00000000\n",
      "epoch: 155 loss: 3661959760191684608.00000000\n",
      "epoch: 157 loss: 3661960035069591552.00000000\n",
      "epoch: 159 loss: 3661960035069591552.00000000\n",
      "epoch: 161 loss: 3661959760191684608.00000000\n",
      "epoch: 163 loss: 3661960309947498496.00000000\n",
      "epoch: 165 loss: 3661959760191684608.00000000\n",
      "epoch: 167 loss: 3661959760191684608.00000000\n",
      "epoch: 169 loss: 3661959760191684608.00000000\n",
      "epoch: 171 loss: 3661960035069591552.00000000\n",
      "epoch: 173 loss: 3661959760191684608.00000000\n",
      "epoch: 175 loss: 3661959760191684608.00000000\n",
      "epoch: 177 loss: 3661960309947498496.00000000\n",
      "epoch: 179 loss: 3661959760191684608.00000000\n",
      "epoch: 181 loss: 3661960035069591552.00000000\n",
      "epoch: 183 loss: 3661960035069591552.00000000\n",
      "epoch: 185 loss: 3661960035069591552.00000000\n",
      "epoch: 187 loss: 3661959760191684608.00000000\n",
      "epoch: 189 loss: 3661960035069591552.00000000\n",
      "epoch: 191 loss: 3661959210435870720.00000000\n",
      "epoch: 193 loss: 3661960035069591552.00000000\n",
      "epoch: 195 loss: 3661959760191684608.00000000\n",
      "epoch: 197 loss: 3661959760191684608.00000000\n",
      "epoch: 199 loss: 3661960035069591552.00000000\n",
      "epoch: 201 loss: 3661959760191684608.00000000\n",
      "epoch: 203 loss: 3661959760191684608.00000000\n",
      "epoch: 205 loss: 3661959760191684608.00000000\n",
      "epoch: 207 loss: 3661960035069591552.00000000\n",
      "epoch: 209 loss: 3661959760191684608.00000000\n",
      "epoch: 211 loss: 3661960035069591552.00000000\n",
      "epoch: 213 loss: 3661959760191684608.00000000\n",
      "epoch: 215 loss: 3661960035069591552.00000000\n",
      "epoch: 217 loss: 3661959760191684608.00000000\n",
      "epoch: 219 loss: 3661959760191684608.00000000\n",
      "epoch: 221 loss: 3661960035069591552.00000000\n",
      "epoch: 223 loss: 3661959760191684608.00000000\n",
      "epoch: 225 loss: 3661960035069591552.00000000\n",
      "epoch: 227 loss: 3661960035069591552.00000000\n",
      "epoch: 229 loss: 3661960035069591552.00000000\n",
      "epoch: 231 loss: 3661960035069591552.00000000\n",
      "epoch: 233 loss: 3661959760191684608.00000000\n",
      "epoch: 235 loss: 3661959760191684608.00000000\n",
      "epoch: 237 loss: 3661959760191684608.00000000\n",
      "epoch: 239 loss: 3661960035069591552.00000000\n",
      "epoch: 241 loss: 3661960035069591552.00000000\n",
      "epoch: 243 loss: 3661960035069591552.00000000\n",
      "epoch: 245 loss: 3661960035069591552.00000000\n",
      "epoch: 247 loss: 3661959760191684608.00000000\n",
      "epoch: 249 loss: 3661960035069591552.00000000\n",
      "epoch: 251 loss: 3661959760191684608.00000000\n",
      "epoch: 253 loss: 3661959760191684608.00000000\n",
      "epoch: 255 loss: 3661960035069591552.00000000\n",
      "epoch: 257 loss: 3661959760191684608.00000000\n",
      "epoch: 259 loss: 3661959760191684608.00000000\n",
      "epoch: 261 loss: 3661959760191684608.00000000\n",
      "epoch: 263 loss: 3661959760191684608.00000000\n",
      "epoch: 265 loss: 3661959760191684608.00000000\n",
      "epoch: 267 loss: 3661960035069591552.00000000\n",
      "epoch: 269 loss: 3661960035069591552.00000000\n",
      "epoch: 271 loss: 3661959760191684608.00000000\n",
      "epoch: 273 loss: 3661960035069591552.00000000\n",
      "epoch: 275 loss: 3661960035069591552.00000000\n",
      "epoch: 277 loss: 3661959760191684608.00000000\n",
      "epoch: 279 loss: 3661959760191684608.00000000\n",
      "epoch: 281 loss: 3661960309947498496.00000000\n",
      "epoch: 283 loss: 3661959760191684608.00000000\n",
      "epoch: 285 loss: 3661959760191684608.00000000\n",
      "epoch: 287 loss: 3661959760191684608.00000000\n",
      "epoch: 289 loss: 3661959760191684608.00000000\n",
      "epoch: 291 loss: 3661959760191684608.00000000\n",
      "epoch: 293 loss: 3661960035069591552.00000000\n",
      "epoch: 295 loss: 3661960035069591552.00000000\n",
      "epoch: 297 loss: 3661959760191684608.00000000\n",
      "epoch: 299 loss: 3661960035069591552.00000000\n",
      "epoch: 301 loss: 3661959760191684608.00000000\n",
      "epoch: 303 loss: 3661959760191684608.00000000\n",
      "epoch: 305 loss: 3661959760191684608.00000000\n",
      "epoch: 307 loss: 3661960035069591552.00000000\n",
      "epoch: 309 loss: 3661959760191684608.00000000\n",
      "epoch: 311 loss: 3661959760191684608.00000000\n",
      "epoch: 313 loss: 3661960035069591552.00000000\n",
      "epoch: 315 loss: 3661959760191684608.00000000\n",
      "epoch: 317 loss: 3661960035069591552.00000000\n",
      "epoch: 319 loss: 3661960035069591552.00000000\n",
      "epoch: 321 loss: 3661959760191684608.00000000\n",
      "epoch: 323 loss: 3661960035069591552.00000000\n",
      "epoch: 325 loss: 3661959760191684608.00000000\n",
      "epoch: 327 loss: 3661960035069591552.00000000\n",
      "epoch: 329 loss: 3661959760191684608.00000000\n",
      "epoch: 331 loss: 3661959760191684608.00000000\n",
      "epoch: 333 loss: 3661959760191684608.00000000\n",
      "epoch: 335 loss: 3661960035069591552.00000000\n",
      "epoch: 337 loss: 3661959760191684608.00000000\n",
      "epoch: 339 loss: 3661959760191684608.00000000\n",
      "epoch: 341 loss: 3661959760191684608.00000000\n",
      "epoch: 343 loss: 3661960035069591552.00000000\n",
      "epoch: 345 loss: 3661959760191684608.00000000\n",
      "epoch: 347 loss: 3661960035069591552.00000000\n",
      "epoch: 349 loss: 3661959760191684608.00000000\n",
      "epoch: 351 loss: 3661960035069591552.00000000\n",
      "epoch: 353 loss: 3661960309947498496.00000000\n",
      "epoch: 355 loss: 3661960035069591552.00000000\n",
      "epoch: 357 loss: 3661960035069591552.00000000\n",
      "epoch: 359 loss: 3661959760191684608.00000000\n",
      "epoch: 361 loss: 3661959760191684608.00000000\n",
      "epoch: 363 loss: 3661959210435870720.00000000\n",
      "epoch: 365 loss: 3661959760191684608.00000000\n",
      "epoch: 367 loss: 3661959760191684608.00000000\n",
      "epoch: 369 loss: 3661959760191684608.00000000\n",
      "epoch: 371 loss: 3661960035069591552.00000000\n",
      "epoch: 373 loss: 3661960035069591552.00000000\n",
      "epoch: 375 loss: 3661960035069591552.00000000\n",
      "epoch: 377 loss: 3661960035069591552.00000000\n",
      "epoch: 379 loss: 3661959760191684608.00000000\n",
      "epoch: 381 loss: 3661960035069591552.00000000\n",
      "epoch: 383 loss: 3661960035069591552.00000000\n",
      "epoch: 385 loss: 3661959760191684608.00000000\n",
      "epoch: 387 loss: 3661959760191684608.00000000\n",
      "epoch: 389 loss: 3661959760191684608.00000000\n",
      "epoch: 391 loss: 3661959760191684608.00000000\n",
      "epoch: 393 loss: 3661960035069591552.00000000\n",
      "epoch: 395 loss: 3661959760191684608.00000000\n",
      "epoch: 397 loss: 3661960035069591552.00000000\n",
      "epoch: 399 loss: 3661960309947498496.00000000\n",
      "epoch: 401 loss: 3661959760191684608.00000000\n",
      "epoch: 403 loss: 3661959760191684608.00000000\n",
      "epoch: 405 loss: 3661959760191684608.00000000\n",
      "epoch: 407 loss: 3661959760191684608.00000000\n",
      "epoch: 409 loss: 3661959760191684608.00000000\n",
      "epoch: 411 loss: 3661959760191684608.00000000\n",
      "epoch: 413 loss: 3661960035069591552.00000000\n",
      "epoch: 415 loss: 3661960035069591552.00000000\n",
      "epoch: 417 loss: 3661959760191684608.00000000\n",
      "epoch: 419 loss: 3661959760191684608.00000000\n",
      "epoch: 421 loss: 3661960035069591552.00000000\n",
      "epoch: 423 loss: 3661959760191684608.00000000\n",
      "epoch: 425 loss: 3661959760191684608.00000000\n",
      "epoch: 427 loss: 3661959760191684608.00000000\n",
      "epoch: 429 loss: 3661960309947498496.00000000\n",
      "epoch: 431 loss: 3661960035069591552.00000000\n",
      "epoch: 433 loss: 3661960035069591552.00000000\n",
      "epoch: 435 loss: 3661959760191684608.00000000\n",
      "epoch: 437 loss: 3661960035069591552.00000000\n",
      "epoch: 439 loss: 3661960035069591552.00000000\n",
      "epoch: 441 loss: 3661960035069591552.00000000\n",
      "epoch: 443 loss: 3661960035069591552.00000000\n",
      "epoch: 445 loss: 3661959760191684608.00000000\n",
      "epoch: 447 loss: 3661960035069591552.00000000\n",
      "epoch: 449 loss: 3661959760191684608.00000000\n",
      "epoch: 451 loss: 3661960035069591552.00000000\n",
      "epoch: 453 loss: 3661959760191684608.00000000\n",
      "epoch: 455 loss: 3661959760191684608.00000000\n",
      "epoch: 457 loss: 3661960035069591552.00000000\n",
      "epoch: 459 loss: 3661960035069591552.00000000\n",
      "epoch: 461 loss: 3661959760191684608.00000000\n",
      "epoch: 463 loss: 3661960035069591552.00000000\n",
      "epoch: 465 loss: 3661959760191684608.00000000\n",
      "epoch: 467 loss: 3661960035069591552.00000000\n",
      "epoch: 469 loss: 3661959760191684608.00000000\n",
      "epoch: 471 loss: 3661959760191684608.00000000\n",
      "epoch: 473 loss: 3661959760191684608.00000000\n",
      "epoch: 475 loss: 3661959760191684608.00000000\n",
      "epoch: 477 loss: 3661959210435870720.00000000\n",
      "epoch: 479 loss: 3661960035069591552.00000000\n",
      "epoch: 481 loss: 3661960035069591552.00000000\n",
      "epoch: 483 loss: 3661959760191684608.00000000\n",
      "epoch: 485 loss: 3661960035069591552.00000000\n",
      "epoch: 487 loss: 3661959760191684608.00000000\n",
      "epoch: 489 loss: 3661959760191684608.00000000\n",
      "epoch: 491 loss: 3661960309947498496.00000000\n",
      "epoch: 493 loss: 3661960035069591552.00000000\n",
      "epoch: 495 loss: 3661959760191684608.00000000\n",
      "epoch: 497 loss: 3661960035069591552.00000000\n",
      "epoch: 499 loss: 3661960035069591552.00000000\n"
     ]
    }
   ],
   "source": [
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 500\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq.float().reshape(-1,1))\n",
    "        single_loss = loss_function(y_pred, labels.float())\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "    if i%2 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T04:15:29.268674400Z",
     "start_time": "2024-01-29T04:15:24.547603800Z"
    }
   },
   "id": "c1b85a351ab23d0"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "\n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "\n",
    "        h_out = h_out.view(-1, self.hidden_size)\n",
    "\n",
    "        out = self.fc(h_out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T04:17:56.015762700Z",
     "start_time": "2024-01-29T04:17:56.014255400Z"
    }
   },
   "id": "49d14bbed070e44f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 2\n",
    "num_layers = 1\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = lstm(trainX)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, trainY)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    data_predict = outputs.data.numpy()\n",
    "    dataY_plot = trainY.data.numpy()\n",
    "    rmse =  np.sqrt(mean_squared_error(dataY_plot,data_predict))\n",
    "    if epoch % 100 == 0:\n",
    "      print(\"Epoch: %d, loss: %1.5f, rmse: %0.f\" % (epoch, loss.item(),rmse))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8e9af9a40170c23"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([14, 1]),\n torch.Size([53, 1]),\n torch.Size([14, 3, 1]),\n torch.Size([53, 3, 1]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train test split ------------------------ Window view def sliding_windows(data, seq_length):\n",
    "def sliding_windows(data, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-seq_length):\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[i+seq_length]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x),np.array(y)\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "training_data = sc.fit_transform(df_exito)\n",
    "\n",
    "seq_length = 3\n",
    "x, y = sliding_windows(training_data, seq_length)\n",
    "\n",
    "train_size = int(len(y) * 0.80)\n",
    "test_size = len(y) - train_size\n",
    "\n",
    "dataX = Variable(torch.Tensor(np.array(x)))\n",
    "dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
    "trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
    "\n",
    "testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n",
    "testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n",
    "testY.shape,trainY.shape,testX.shape,trainX.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T15:15:11.466820Z",
     "start_time": "2024-01-28T15:15:11.449806200Z"
    }
   },
   "id": "898c5a1b9335043e"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.3174],\n        [0.2468],\n        [0.1605],\n        [0.0995],\n        [0.1764],\n        [0.1401],\n        [0.2064],\n        [0.4278],\n        [0.1279],\n        [0.2611],\n        [0.1856],\n        [0.1962],\n        [0.3636],\n        [0.2601],\n        [0.2871],\n        [0.2533],\n        [0.3514],\n        [0.2492],\n        [0.3140],\n        [0.6192],\n        [0.2158],\n        [0.3209],\n        [0.1200],\n        [0.1396],\n        [0.2704],\n        [0.1192],\n        [0.0730],\n        [0.0000],\n        [0.2352],\n        [0.1631],\n        [0.1040],\n        [0.4438],\n        [0.1880],\n        [0.3241],\n        [0.1837],\n        [0.0795],\n        [0.2009],\n        [0.1815],\n        [0.3040],\n        [0.1033],\n        [0.6243],\n        [0.2903],\n        [0.3686],\n        [0.8182],\n        [0.2151],\n        [0.3058],\n        [0.2943],\n        [0.3169],\n        [0.4000],\n        [0.4388],\n        [0.4645],\n        [0.2824],\n        [0.4954]])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T02:31:26.528260700Z",
     "start_time": "2024-01-28T02:31:26.514895800Z"
    }
   },
   "id": "c6b6bfb02cfa1557"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.20344741],\n       [0.22832548],\n       [0.16494633],\n       [0.3174342 ],\n       [0.24684165],\n       [0.16051058],\n       [0.09951947],\n       [0.1763594 ],\n       [0.14006367],\n       [0.20636404],\n       [0.42779528],\n       [0.12790886],\n       [0.26106697],\n       [0.18563728],\n       [0.1961531 ],\n       [0.36360709],\n       [0.26014296],\n       [0.28711494],\n       [0.25332519],\n       [0.35141444],\n       [0.24917775],\n       [0.31404204],\n       [0.61917257],\n       [0.21582119],\n       [0.32087431],\n       [0.119998  ],\n       [0.13962141],\n       [0.27035317],\n       [0.11915673],\n       [0.07298417],\n       [0.        ],\n       [0.23518747],\n       [0.16313112],\n       [0.10399878],\n       [0.44384596],\n       [0.18797634],\n       [0.32413378],\n       [0.18374765],\n       [0.07953415],\n       [0.2008586 ],\n       [0.18148693],\n       [0.30395526],\n       [0.10330469],\n       [0.62425264],\n       [0.29025822],\n       [0.36857542],\n       [0.81822425],\n       [0.21510477],\n       [0.30584151],\n       [0.29425535],\n       [0.31694715],\n       [0.40000301],\n       [0.43882987],\n       [0.46451416],\n       [0.28243186],\n       [0.49541073],\n       [0.36474415],\n       [0.44091569],\n       [1.        ],\n       [0.27972124],\n       [0.40307008],\n       [0.49787948],\n       [0.29765171],\n       [0.4295788 ],\n       [0.40741613],\n       [0.55149682],\n       [0.27245608],\n       [0.52815394],\n       [0.31326672],\n       [0.51850741]])"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T02:27:31.586019600Z",
     "start_time": "2024-01-28T02:27:31.583906100Z"
    }
   },
   "id": "4f63177489531b5e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m---> 21\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;66;03m# obtain the loss function\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\OneDrive\\Escritorio\\Machine_Learning\\mlflow\\model.py:40\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     37\u001B[0m ula, (h_out, _) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm(x, (h_0, c_0))\n\u001B[0;32m     39\u001B[0m h_out \u001B[38;5;241m=\u001B[39m h_out\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_size)\n\u001B[1;32m---> 40\u001B[0m h_out \u001B[38;5;241m=\u001B[39m \u001B[43mh_out\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     41\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(h_out)\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[1;31mIndexError\u001B[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(40)\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "\n",
    "criterion = torch.nn.MSELoss()  # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = lstm(trainX)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, trainY)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    data_predict = outputs.data.numpy()\n",
    "    dataY_plot = trainY.data.numpy()\n",
    "\n",
    "    data_predict = sc.inverse_transform(data_predict.reshape(-1,1))\n",
    "    dataY_plot = sc.inverse_transform(dataY_plot.reshape(-1,1))\n",
    "    print(data_predict.shape)\n",
    "    print(dataY_plot.shape)\n",
    "    #rmse = np.sqrt(mean_squared_error(dataY_plot, data_predict))\n",
    "    #mse = mean_squared_error(dataY_plot, data_predict)\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f, rmse: %0.f\" % (epoch, mse, rmse))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T15:15:16.752798400Z",
     "start_time": "2024-01-28T15:15:15.921165400Z"
    }
   },
   "id": "9987c7e96c602d95"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.3272138e+09]], dtype=float32)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.inverse_transform(data_predict.reshape(-1,1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T15:01:26.211638500Z",
     "start_time": "2024-01-28T15:01:26.207211800Z"
    }
   },
   "id": "dea7590e3aa09e7f"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([159, 1])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(40)\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "lstm(trainX).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:56:59.145209700Z",
     "start_time": "2024-01-28T14:56:59.107597200Z"
    }
   },
   "id": "2cc33c044862c4f4"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "159"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53 * 3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T14:49:00.331573700Z",
     "start_time": "2024-01-28T14:49:00.326723200Z"
    }
   },
   "id": "cfc151608e3ba0bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
